{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "\n",
    "import onnx\n",
    "import yaml\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.mask import iou, encode\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from onnx import numpy_helper\n",
    "import os\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# key = COCO id, value = Pascal VOC id\n",
    "COCO_TO_VOC = {\n",
    "    1: 15,  # person\n",
    "    2: 2,   # bicycle\n",
    "    3: 7,   # car\n",
    "    4: 14,  # motorbike\n",
    "    5: 1,   # airplane\n",
    "    6: 6,   # bus\n",
    "    7: 19,  # train\n",
    "    9: 4,   # boat\n",
    "    16: 3,  # bird\n",
    "    17: 8,  # cat\n",
    "    18: 12, # dog\n",
    "    19: 13, # horse\n",
    "    20: 17, # sheep\n",
    "    21: 10, # cow\n",
    "    44: 5,  # bottle\n",
    "    62: 9,  # chair\n",
    "    63: 18, # couch/sofa\n",
    "    64: 16, # potted plant\n",
    "    67: 11, # dining table\n",
    "    72: 20, # tv\n",
    "}\n",
    "\n",
    "VOC_CAT_IDS = list(COCO_TO_VOC.keys())\n",
    "cocoGt = COCO(str(\"data/annotations/instances_val2017.json\"))\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        imgIds = self.getImgIdsUnion(cocoGt, VOC_CAT_IDS)\n",
    "        self.data = []\n",
    "        for imgId in imgIds:\n",
    "            img_path = os.path.join(\"data/val2017_sub/\", cocoGt.imgs[imgId]['file_name'])\n",
    "            if os.path.exists(img_path):\n",
    "                input_tensor = self.load_image(img_path)\n",
    "                \n",
    "                _, height, width = input_tensor.shape\n",
    "                output_tensor = np.zeros((21, height, width), dtype=np.uint8)\n",
    "                \n",
    "                annIds = cocoGt.getAnnIds(imgId, VOC_CAT_IDS)\n",
    "                for ann in cocoGt.loadAnns(annIds):\n",
    "                    mask = cocoGt.annToMask(ann)\n",
    "                    output_tensor[COCO_TO_VOC[ann['category_id']]] |= mask\n",
    "                    \n",
    "                # Set everything not labeled to be background\n",
    "                output_tensor[0] = 1 - np.max(output_tensor, axis=0)\n",
    "                self.data.append((input_tensor, output_tensor))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def getImgIdsUnion(self, gt, catIds):\n",
    "        \"\"\"\n",
    "        Returns all the images that have *any* of the categories in `catIds`,\n",
    "        unlike the built-in `gt.getImgIds` which returns all the images containing\n",
    "        *all* of the categories in `catIds`.\n",
    "        \"\"\"\n",
    "        imgIds = set()\n",
    "        for catId in catIds:\n",
    "            imgIds |= set(gt.catToImgs[catId])\n",
    "        return list(imgIds)\n",
    "        \n",
    "    def load_image(self, img_path):\n",
    "        input_image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = preprocess(input_image)\n",
    "        input_tensor = input_tensor.detach().cpu().numpy()\n",
    "        return input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(model_tensor, target_tensor):\n",
    "    # Don't include the background when summing\n",
    "    model_tensor = model_tensor[:, 1:, :, :]\n",
    "    target_tensor = target_tensor[:, 1:, :, :]\n",
    "    \n",
    "    intersection = np.sum(np.logical_and(model_tensor, target_tensor))\n",
    "    union = np.sum(np.logical_or(model_tensor, target_tensor))\n",
    "    \n",
    "    if union == 0:\n",
    "        # Can only happen if nothing was there and nothing was predicted,\n",
    "        # which is a perfect score\n",
    "        return 1\n",
    "    else:\n",
    "        return intersection / union\n",
    "\n",
    "def evaluate(model, dataloader):    \n",
    "    totalIoU = 0\n",
    "    sess = onnxruntime.InferenceSession(model.SerializeToString(),\n",
    "                                        None,\n",
    "                                        providers=onnxruntime.get_available_providers())\n",
    "    idx = 1\n",
    "    for input_tensor, target_tensor in dataloader:\n",
    "        input_tensor = input_tensor[np.newaxis, ...]\n",
    "        target_tensor = target_tensor[np.newaxis, ...]\n",
    "        model_tensor = sess.run([\"out\"], {\"input\": input_tensor})[0]\n",
    "        \n",
    "        batch_size, nclasses, height, width = model_tensor.shape\n",
    "        raw_labels = np.argmax(model_tensor, axis=1).astype(np.uint8)\n",
    "        \n",
    "        output_tensor = np.zeros((nclasses, batch_size, height, width), dtype=np.uint8)\n",
    "        for c in range(nclasses):\n",
    "            output_tensor[c][raw_labels==c] = 1\n",
    "\n",
    "        output_tensor = np.transpose(output_tensor, [1, 0, 2, 3])          \n",
    "        totalIoU += iou(output_tensor, target_tensor)    \n",
    "        idx += 1\n",
    "    return totalIoU / idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "  return evaluate(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"models/fcn-resnet50-12.onnx\")\n",
    "ds = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:01:58 [INFO] Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-12-16 23:01:58 [INFO] NumExpr defaulting to 8 threads.\n",
      "2022-12-16 23:01:59.932019: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 23:02:00.077560: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-16 23:02:00.526678: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nana/anaconda3/envs/tf/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-12-16 23:02:00.526735: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nana/anaconda3/envs/tf/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-12-16 23:02:00.526739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-16 23:02:03 [INFO] Get FP32 model baseline.\n",
      "2022-12-16 23:02:16 [INFO] Save tuning history to /home/nana/Documents/PDL/Final Project/Final-Project-Semantic-Segmentation/FCN-Quantization/nc_workspace/2022-12-16_23-01-59/./history.snapshot.\n",
      "2022-12-16 23:02:16 [INFO] FP32 baseline is: [Accuracy: 0.6108, Duration (seconds): 12.6811]\n",
      "2022-12-16 23:02:16 [WARNING] onnxrt_qlinearops uses the same model representation format as onnxrt_qoperator. Recommended to use onnxrt_qoperator to align with ONNXRUNTIME QuantFormat\n",
      "2022-12-16 23:03:17 [INFO] |*********Mixed Precision Statistics*********|\n",
      "2022-12-16 23:03:17 [INFO] +-------------------+--------+-------+-------+\n",
      "2022-12-16 23:03:17 [INFO] |      Op Type      | Total  |  INT8 |  FP32 |\n",
      "2022-12-16 23:03:17 [INFO] +-------------------+--------+-------+-------+\n",
      "2022-12-16 23:03:17 [INFO] |        Conv       |   57   |   57  |   0   |\n",
      "2022-12-16 23:03:17 [INFO] |       Gather      |   2    |   0   |   2   |\n",
      "2022-12-16 23:03:17 [INFO] |      MaxPool      |   1    |   1   |   0   |\n",
      "2022-12-16 23:03:17 [INFO] |        Add        |   16   |   16  |   0   |\n",
      "2022-12-16 23:03:17 [INFO] |       Concat      |   3    |   0   |   3   |\n",
      "2022-12-16 23:03:17 [INFO] |     Unsqueeze     |   2    |   0   |   2   |\n",
      "2022-12-16 23:03:17 [INFO] |       Resize      |   2    |   2   |   0   |\n",
      "2022-12-16 23:03:17 [INFO] |   QuantizeLinear  |   1    |   1   |   0   |\n",
      "2022-12-16 23:03:17 [INFO] |  DequantizeLinear |   4    |   4   |   0   |\n",
      "2022-12-16 23:03:17 [INFO] +-------------------+--------+-------+-------+\n",
      "2022-12-16 23:03:17 [INFO] Pass quantize model elapsed time: 61072.67 ms\n",
      "2022-12-16 23:03:27 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.6045|0.6108, Duration (seconds) (int8|fp32): 10.3128|12.6811], Best tune result is: [Accuracy: 0.6045, Duration (seconds): 10.3128]\n",
      "2022-12-16 23:03:27 [INFO] |**********************Tune Result Statistics**********************|\n",
      "2022-12-16 23:03:27 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-12-16 23:03:27 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
      "2022-12-16 23:03:27 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-12-16 23:03:27 [INFO] |      Accuracy      | 0.6108   |    0.6045     |     0.6045       |\n",
      "2022-12-16 23:03:27 [INFO] | Duration (seconds) | 12.6811  |    10.3128    |     10.3128      |\n",
      "2022-12-16 23:03:27 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-12-16 23:03:27 [INFO] Save tuning history to /home/nana/Documents/PDL/Final Project/Final-Project-Semantic-Segmentation/FCN-Quantization/nc_workspace/2022-12-16_23-01-59/./history.snapshot.\n",
      "2022-12-16 23:03:27 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2022-12-16 23:03:27 [INFO] Save deploy yaml to /home/nana/Documents/PDL/Final Project/Final-Project-Semantic-Segmentation/FCN-Quantization/nc_workspace/2022-12-16_23-01-59/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "from neural_compressor.experimental import Quantization, common\n",
    "from neural_compressor import options\n",
    "options.onnxrt.graph_optimization.level = 'ENABLE_BASIC'\n",
    "\n",
    "quantize = Quantization(\"fcn_rn50.yaml\")\n",
    "quantize.model = common.Model(model)\n",
    "quantize.calib_dataloader = common.DataLoader(ds)\n",
    "quantize.eval_func = eval\n",
    "q_model = quantize()\n",
    "q_model.save(\"models/quantized/fcn-resnet50-int8.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:05:56 [INFO] Start to run Benchmark.\n",
      "2022-12-16 23:06:06 [INFO] \n",
      "performance mode benchmark result:\n",
      "2022-12-16 23:06:06 [INFO] Batch size = 1\n",
      "2022-12-16 23:06:06 [INFO] Latency: 353.151 ms\n",
      "2022-12-16 23:06:06 [INFO] Throughput: 2.832 images/sec\n"
     ]
    }
   ],
   "source": [
    "from neural_compressor.experimental import Benchmark, common\n",
    "\n",
    "model = onnx.load(\"models/fcn-resnet50-12.onnx\")\n",
    "\n",
    "evaluator = Benchmark(\"fcn_rn50.yaml\")\n",
    "evaluator.model = common.Model(model)\n",
    "evaluator.b_dataloader = common.DataLoader(ds)\n",
    "evaluator(\"performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:06:45 [INFO] Start to run Benchmark.\n",
      "2022-12-16 23:06:53 [INFO] \n",
      "performance mode benchmark result:\n",
      "2022-12-16 23:06:53 [INFO] Batch size = 1\n",
      "2022-12-16 23:06:53 [INFO] Latency: 289.919 ms\n",
      "2022-12-16 23:06:53 [INFO] Throughput: 3.449 images/sec\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"models/quantized/fcn-resnet50-int8.onnx\")\n",
    "\n",
    "evaluator = Benchmark(\"fcn_rn50.yaml\")\n",
    "evaluator.model = common.Model(model)\n",
    "evaluator.b_dataloader = common.DataLoader(ds)\n",
    "evaluator(\"performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03278fafecf26aeb7ce7d788a5986a21b938eef97c5253238ae978c530476e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
