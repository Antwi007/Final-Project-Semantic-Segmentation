{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "\n",
    "import onnx\n",
    "import yaml\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.mask import iou, encode\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from onnx import numpy_helper\n",
    "import os\n",
    "import onnxruntime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, root, img_dir='val2017', \\\n",
    "            anno_dir='annotations/instances_val2017.json'):\n",
    "        import json\n",
    "        import os\n",
    "        import numpy as np\n",
    "        from pycocotools.coco import COCO\n",
    "        from neural_compressor.experimental.metric.coco_label_map import category_map\n",
    "        self.batch_size = 1\n",
    "        self.image_list = []\n",
    "        img_path = os.path.join(root, img_dir)\n",
    "        anno_path = os.path.join(root, anno_dir)\n",
    "        coco = COCO(anno_path)\n",
    "        img_ids = coco.getImgIds()\n",
    "        cat_ids = coco.getCatIds()\n",
    "        for idx, img_id in enumerate(img_ids):\n",
    "            img_info = {}\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            ids = []\n",
    "            img_detail = coco.loadImgs(img_id)[0]\n",
    "            ids.append(img_detail['file_name'].encode('utf-8'))\n",
    "            pic_height = img_detail['height']\n",
    "            pic_width = img_detail['width']\n",
    "\n",
    "            ann_ids = coco.getAnnIds(imgIds=img_id,catIds=cat_ids)\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                bbox = ann['bbox']\n",
    "                if len(bbox) == 0:\n",
    "                    continue\n",
    "                bboxes.append([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])\n",
    "                labels.append(category_map[ann['category_id']].encode('utf8'))\n",
    "            img_file = os.path.join(img_path, img_detail['file_name'])\n",
    "            if not os.path.exists(img_file) or len(bboxes) == 0:\n",
    "                continue\n",
    "\n",
    "            if filter and not filter(None, bboxes):\n",
    "                continue\n",
    "            label = [np.array([bboxes]), np.array([labels]), np.zeros((1,0)), np.array([img_detail['file_name'].encode('utf-8')])]\n",
    "            with Image.open(img_file) as image:\n",
    "                image = image.convert('RGB')\n",
    "                image, label = self.preprocess((image, label))\n",
    "            self.image_list.append((image, label))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.image_list:\n",
    "            yield item\n",
    "\n",
    "    def preprocess(self, sample):\n",
    "        image, label = sample\n",
    "        ratio = 800.0 / min(image.size[0], image.size[1])\n",
    "        image = image.resize((int(ratio * image.size[0]), int(ratio * image.size[1])), Image.BILINEAR)\n",
    "\n",
    "        # Convert to BGR\n",
    "        image = np.array(image)[:, :, [2, 1, 0]].astype('float32')\n",
    "\n",
    "        # HWC -> CHW\n",
    "        image = np.transpose(image, [2, 0, 1])\n",
    "\n",
    "        # Normalize\n",
    "        mean_vec = np.array([102.9801, 115.9465, 122.7717])\n",
    "        for i in range(image.shape[0]):\n",
    "            image[i, :, :] = image[i, :, :] - mean_vec[i]\n",
    "\n",
    "        # Pad to be divisible of 32\n",
    "        padded_h = int(math.ceil(image.shape[1] / 32) * 32)\n",
    "        padded_w = int(math.ceil(image.shape[2] / 32) * 32)\n",
    "\n",
    "        padded_image = np.zeros((3, padded_h, padded_w), dtype=np.float32)\n",
    "        padded_image[:, :image.shape[1], :image.shape[2]] = image\n",
    "        image = padded_image\n",
    "        bboxes, str_labels,int_labels, image_ids = label\n",
    "        bboxes = ratio * bboxes\n",
    "        return image, (bboxes, str_labels, int_labels, image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"models/MaskRCNN-12.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    def __call__(self, sample):\n",
    "        preds, labels = sample\n",
    "        bboxes, classes, scores, _ = preds\n",
    "        bboxes = np.reshape(bboxes, (1, -1, 4))\n",
    "        classes = np.reshape(classes, (1, -1))\n",
    "        scores = np.reshape(scores, (1, -1))\n",
    "        return (bboxes, classes, scores), labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 00:05:47 [INFO] Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-12-19 00:05:47 [INFO] NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_442278/2091554627.py:53: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  image = image.resize((int(ratio * image.size[0]), int(ratio * image.size[1])), Image.BILINEAR)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from neural_compressor import options\n",
    "from neural_compressor.experimental import Quantization, common\n",
    "options.onnxrt.graph_optimization.level = 'ENABLE_BASIC'\n",
    "\n",
    "model = onnx.load(\"models/MaskRCNN-12.onnx\")\n",
    "dataloader = Dataloader(\"data/\")\n",
    "\n",
    "quantize = Quantization(\"mask_rcnn.yaml\")\n",
    "quantize.model = common.Model(model)\n",
    "quantize.eval_dataloader = dataloader\n",
    "quantize.calib_dataloader = dataloader\n",
    "quantize.postprocess = common.Postprocess(Post)\n",
    "q_model = quantize()\n",
    "q_model.save(\"models/quantized/mask_rcnn.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above memory requirement is large so Quantizing Model doesn't work locally on a 32 GB Memory PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03278fafecf26aeb7ce7d788a5986a21b938eef97c5253238ae978c530476e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
